---
title: "Gemini Chat (Compatible)"
description: "Access Google Gemini models using the OpenAI-compatible chat interface."
---

<Note>
  ChainHub allows you to access Google Gemini models (like Gemini 1.5 Pro, Gemini 1.5 Flash) using the standard OpenAI-compatible chat completion format.
</Note>

### Endpoint

<ParamField header="Authorization" type="string" required placeholder="Bearer {{YOUR_API_KEY}}">
  Your ChainHub API Key.
</ParamField>

<ParamField body="model" type="string" required>
  The ID of the Gemini model to use (e.g., `gemini-1.5-pro`, `gemini-1.5-flash`).
</ParamField>

<ParamField body="messages" type="array" required>
  A list of messages comprising the conversation so far.
  <Expandable title="Message Object">
    <ParamField body="role" type="string" required>
      The role of the message author. Supported roles: `system`, `user`, `assistant`.
    </ParamField>
    <ParamField body="content" type="string" required>
      The contents of the message.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="temperature" type="number" default="1.0">
  What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
</ParamField>

<ParamField body="top_p" type="number" default="1.0">
  An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
</ParamField>

<ParamField body="n" type="integer" default="1">
  How many chat completion choices to generate for each input message.
</ParamField>

<ParamField body="stream" type="boolean" default="false">
  If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available.
</ParamField>

<ParamField body="max_tokens" type="integer">
  The maximum number of tokens to generate in the chat completion.
</ParamField>

<ParamField body="presence_penalty" type="number" default="0">
  Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far.
</ParamField>

<ParamField body="frequency_penalty" type="number" default="0">
  Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far.
</ParamField>

<ParamField body="response_format" type="object">
  An object specifying the format that the model must output.
  <Expandable title="Response Format Object">
    <ParamField body="type" type="string">
      Must be `json_object` to enable JSON mode.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="tools" type="array">
  A list of tools the model may call.
</ParamField>

<ParamField body="tool_choice" type="string | object">
  Controls which (if any) tool is called by the model.
</ParamField>

<ResponseExample>
```json Request
{
  "model": "gemini-1.5-pro",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "Hello!"
    }
  ],
  "temperature": 0.7
}
```

```json Response
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21
  }
}
```
</ResponseExample>
